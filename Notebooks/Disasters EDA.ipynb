{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54740f1-92b0-44f3-b72b-9c30d0cafbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from access_credentials.gcp_credentials import GOOGLE_MAPS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736eeead-318e-4950-8c79-67aecea101e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"access_credentials/rsp-dm-ii-dv-iii-elt-flow.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945d0088-5116-4787-8e49-3687293ab392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Datasets/RawDatasets/public_emdat_natural.xlsx', sheet_name='EM-DAT Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0004c36-d8b6-42cd-87ea-6c834ea36e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2104 entries, 0 to 2103\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   DisNo.                                     2104 non-null   object \n",
      " 1   Historic                                   2104 non-null   object \n",
      " 2   Classification Key                         2104 non-null   object \n",
      " 3   Disaster Group                             2104 non-null   object \n",
      " 4   Disaster Subgroup                          2104 non-null   object \n",
      " 5   Disaster Type                              2104 non-null   object \n",
      " 6   Disaster Subtype                           2104 non-null   object \n",
      " 7   External IDs                               213 non-null    object \n",
      " 8   Event Name                                 310 non-null    object \n",
      " 9   ISO                                        2104 non-null   object \n",
      " 10  Country                                    2104 non-null   object \n",
      " 11  Subregion                                  2104 non-null   object \n",
      " 12  Region                                     2104 non-null   object \n",
      " 13  Location                                   1754 non-null   object \n",
      " 14  Origin                                     445 non-null    object \n",
      " 15  Associated Types                           413 non-null    object \n",
      " 16  OFDA Response                              2104 non-null   object \n",
      " 17  Appeal                                     2104 non-null   object \n",
      " 18  Declaration                                2104 non-null   object \n",
      " 19  AID Contribution ('000 US$)                53 non-null     float64\n",
      " 20  Magnitude                                  717 non-null    float64\n",
      " 21  Magnitude Scale                            1981 non-null   object \n",
      " 22  Latitude                                   304 non-null    float64\n",
      " 23  Longitude                                  304 non-null    float64\n",
      " 24  River Basin                                243 non-null    object \n",
      " 25  Start Year                                 2104 non-null   int64  \n",
      " 26  Start Month                                2081 non-null   float64\n",
      " 27  Start Day                                  1760 non-null   float64\n",
      " 28  End Year                                   2104 non-null   int64  \n",
      " 29  End Month                                  2060 non-null   float64\n",
      " 30  End Day                                    1745 non-null   float64\n",
      " 31  Total Deaths                               1281 non-null   float64\n",
      " 32  No. Injured                                398 non-null    float64\n",
      " 33  No. Affected                               841 non-null    float64\n",
      " 34  No. Homeless                               156 non-null    float64\n",
      " 35  Total Affected                             1094 non-null   float64\n",
      " 36  Reconstruction Costs ('000 US$)            6 non-null      float64\n",
      " 37  Reconstruction Costs, Adjusted ('000 US$)  6 non-null      float64\n",
      " 38  Insured Damage ('000 US$)                  238 non-null    float64\n",
      " 39  Insured Damage, Adjusted ('000 US$)        237 non-null    float64\n",
      " 40  Total Damage ('000 US$)                    780 non-null    float64\n",
      " 41  Total Damage, Adjusted ('000 US$)          775 non-null    float64\n",
      " 42  CPI                                        2071 non-null   float64\n",
      " 43  Admin Units                                1122 non-null   object \n",
      " 44  Entry Date                                 2104 non-null   object \n",
      " 45  Last Update                                2104 non-null   object \n",
      "dtypes: float64(20), int64(2), object(24)\n",
      "memory usage: 756.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad871af-c8bf-4025-a87f-0f169b921d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CSV for the CPI Table\n",
    "\n",
    "cpi_list = list()\n",
    "for year in df['Start Year'].unique():\n",
    "    cpi_list.append({\n",
    "        'CPICode': f'C_{year}',\n",
    "        'Year': year,\n",
    "        'Value': df[df['Start Year'] == year]['CPI'].values[0]\n",
    "    })\n",
    "\n",
    "cpi_df = pd.DataFrame(cpi_list)\n",
    "cpi_df.sort_values('Year', inplace=True)\n",
    "cpi_df.fillna(128.11, inplace=True)\n",
    "cpi_df.to_csv('Datasets/CleanedDatasets/CPI.csv', sep='|', index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7ac5b0-6b1c-415f-982c-a6a57d40f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CSV for the Country Table\n",
    "country_df = df[['ISO', 'Region', 'Subregion', 'Country']].drop_duplicates()\n",
    "country_df.columns = ['ISOCode', 'Region', 'Subregion', 'Country']\n",
    "country_df.sort_values('ISOCode', inplace=True)\n",
    "country_df.to_csv('Datasets/CleanedDatasets/Country.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e902ec7-cbed-4af3-99c9-7a054aa24c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CSV for the ExternalReqRes Table\n",
    "\n",
    "ext_req_res_list = []\n",
    "for ofda in [0, 1]:\n",
    "    for appeal in [0, 1]:\n",
    "        for declaration in [0, 1]:\n",
    "            ext_req_res_dict = {'ExternalReqResId': len(ext_req_res_list) + 1, 'OFDA': ofda, 'Appeal': appeal, 'Declaration': declaration}\n",
    "            ext_req_res_list.append(ext_req_res_dict)\n",
    "\n",
    "external_req_res_df = pd.DataFrame(ext_req_res_list)\n",
    "external_req_res_df.sort_values('ExternalReqResId', inplace=True)\n",
    "external_req_res_df.to_csv('Datasets/CleanedDatasets/ExternalReqRes.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0043ef4e-d86a-48a3-9973-fb151f150993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CSV for DisasterClassification Table\n",
    "\n",
    "disaster_classification_cols = ['Classification Key', 'Disaster Group', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype', 'Magnitude Scale']\n",
    "disaster_classification = df[disaster_classification_cols].drop_duplicates()\n",
    "disaster_classification.columns = ['ClassificationKey', 'Group', 'Subgroup', 'Type', 'Subtype', 'Unit']\n",
    "disaster_classification.sort_values('ClassificationKey', inplace=True)\n",
    "disaster_classification.to_csv('Datasets/CleanedDatasets/DisasterClassification.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fbb103-bda7-4c0c-ae0a-0cb1d592412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CSV for AssociateType Table\n",
    "\n",
    "def replace_associate_type(assoc_types):\n",
    "    if not isinstance(assoc_types, str):\n",
    "        return assoc_types\n",
    "\n",
    "    replacements = {\n",
    "        'Snow/ice': 'Snow',\n",
    "        'Avalanche (Snow, Debris)': 'Avalanche',\n",
    "        'Broken Dam/Burst bank': 'Burst dam or bank',\n",
    "        'Tsunami/Tidal wave': 'Tidal wave',\n",
    "        'Slide (land, mud, snow, rock)': 'Land slide',\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        assoc_types = assoc_types.replace(old, new)\n",
    "\n",
    "    return assoc_types\n",
    "\n",
    "df['Associated Types'] = df['Associated Types'].apply(replace_associate_type)\n",
    "\n",
    "unique_associated_types = set()\n",
    "for types in df['Associated Types'].unique():\n",
    "    if isinstance(types, str):\n",
    "        unique_associated_types.update(types.split(\"|\"))\n",
    "\n",
    "unique_associated_types = sorted(list(unique_associated_types))\n",
    "\n",
    "associate_type_list = [{'AssociateTypeId': index + 1, 'AssociateType': assoc_type}\n",
    "                      for index, assoc_type in enumerate(unique_associated_types)]\n",
    "associate_type_df = pd.DataFrame(associate_type_list)\n",
    "associate_type_df.sort_values('AssociateTypeId', inplace=True)\n",
    "associate_type_df.to_csv('Datasets/CleanedDatasets/AssociateType.csv', sep='|', index=False)\n",
    "\n",
    "# Creating CSV for DisasterAssociate Table (Which is a connection between Disaster and AssociateType Table)\n",
    "\n",
    "associate_type_df = pd.read_csv('Datasets/CleanedDatasets/AssociateType.csv', sep='|')\n",
    "\n",
    "disaster_associate_list = list()\n",
    "for associate in associate_type_df.iterrows():\n",
    "    associate_type_id = associate[1]['AssociateTypeId']\n",
    "    associate_type = associate[1]['AssociateType']\n",
    "    for row in df[~df['Associated Types'].isna()].iterrows():\n",
    "        if associate_type in str(row[1]['Associated Types']):\n",
    "            disaster_associate_dict = {\n",
    "                'DisasterAssociateId': len(disaster_associate_list) + 1,\n",
    "                'DisasterNo': row[1]['DisNo.'],\n",
    "                'AssociateTypeId': associate_type_id,\n",
    "            }\n",
    "\n",
    "            disaster_associate_list.append(disaster_associate_dict)\n",
    "\n",
    "disaster_associate = pd.DataFrame(disaster_associate_list)\n",
    "disaster_associate.to_csv('Datasets/CleanedDatasets/DisasterAssociate.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf489024-9112-4286-8871-73df60efa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Newly Found Locations into the df-dataframe.\n",
    "location_dataframe = pd.read_csv('Datasets/IntermediateDatasets/location_filled.csv', sep=\";\")\n",
    "\n",
    "merged_df = df.merge(location_dataframe[['DisNo.', 'Location']], on='DisNo.', how='left')\n",
    "merged_df['Location'] = merged_df['Location_y'].combine_first(merged_df['Location_x'])\n",
    "merged_df = merged_df.drop(['Location_x', 'Location_y'], axis=1)\n",
    "\n",
    "df = df.merge(merged_df[['DisNo.', 'Location']], on='DisNo.', how='left')\n",
    "df['Location'] = df['Location_x'].combine_first(df['Location_y'])\n",
    "df = df.drop(['Location_x', 'Location_y'], axis=1)\n",
    "\n",
    "# Using Google Geocoding API to find out the Lat and Lon of the Locations\n",
    "\n",
    "gmaps = googlemaps.Client(key=GOOGLE_MAPS_KEY)\n",
    "\n",
    "for row in df.iterrows():\n",
    "    dis_no = row[1]['DisNo.']\n",
    "    file_path = f'Datasets/IntermediateDatasets/GeocodedJsonFiles/{dis_no}.json'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            geocode_result = gmaps.geocode(f\"({row[1]['Location']}) in {row[1]['Country']}\")\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(json.dumps(geocode_result))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Creating CSV for AssociateType Table\n",
    "\n",
    "location_details_list = list()\n",
    "for row in df[~df.Location.isna()].iterrows():\n",
    "    dis_no = row[1]['DisNo.']\n",
    "    file_path = f'Datasets/IntermediateDatasets/GeocodedJsonFiles/{dis_no}.json'\n",
    "    geo_coding = json.loads(open(file_path).read())\n",
    "\n",
    "    for loc in geo_coding:\n",
    "        location_dict = {\n",
    "            'LocationID': len(location_details_list) + 1,\n",
    "            'DisasterNo': dis_no,\n",
    "            'Location': loc['formatted_address'],\n",
    "            'Latitude': loc['geometry']['location']['lat'],\n",
    "            'Longitude': loc['geometry']['location']['lng']\n",
    "        }\n",
    "\n",
    "        location_details_list.append(location_dict)\n",
    "\n",
    "location_details_dataframe = pd.DataFrame(location_details_list)\n",
    "location_details_dataframe.to_csv('Datasets/CleanedDatasets/Location.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d3219f-be21-4580-8721-21d568e8d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_req_res_df = pd.read_csv('Datasets/CleanedDatasets/ExternalReqRes.csv', sep='|')\n",
    "\n",
    "def fetch_ext_req_res_code(row):\n",
    "    ofda_con = ext_req_res_df['OFDA'] == (row[1]['OFDA Response'] == 'Yes')\n",
    "    appeal_con = ext_req_res_df['Appeal'] == (row[1]['Appeal'] == 'Yes')\n",
    "    declaration_con = ext_req_res_df['Declaration'] == (row[1]['Declaration'] == 'Yes')\n",
    "    ext_req_res_row = ext_req_res_df[ofda_con & appeal_con & declaration_con]\n",
    "    \n",
    "    return ext_req_res_row['ExternalReqResId'].values[0]\n",
    "\n",
    "disaster_list = list()\n",
    "for row in df.iterrows():\n",
    "    disaster_list.append({\n",
    "        'DisasterId': len(disaster_list) + 1,\n",
    "        'DisasterNum': row[1]['DisNo.'],\n",
    "        'ClassificationKey': row[1]['Classification Key'],\n",
    "        'ISOCode': row[1]['ISO'],\n",
    "        'ExternalReqResId': fetch_ext_req_res_code(row),\n",
    "        'EventName': row[1]['Event Name'],\n",
    "        'RiverBasin': row[1]['River Basin'],\n",
    "        'DisasterOrigin': row[1]['Origin'],\n",
    "        'DisasterMagnitude': row[1]['Magnitude'],\n",
    "        'AidContribution': row[1]['AID Contribution (\\'000 US$)'],\n",
    "        'StartYear': row[1]['Start Year'],\n",
    "        'StartMonth': row[1]['Start Month'],\n",
    "        'StartDay': row[1]['Start Day'],\n",
    "        'EndYear': row[1]['End Year'],\n",
    "        'EndMonth': row[1]['End Month'],\n",
    "        'EndDay': row[1]['End Day'],\n",
    "        'TotalDeaths': row[1]['Total Deaths'],\n",
    "        'NumInjured': row[1]['No. Injured'],\n",
    "        'NumAffected': row[1]['No. Affected'],\n",
    "        'NumHomeless': row[1]['No. Homeless'],\n",
    "        'TotalAffected': row[1]['Total Affected'],\n",
    "        'ReconstructionCost': row[1]['Reconstruction Costs (\\'000 US$)'],\n",
    "        'ReconstructionCostAdj': row[1]['Reconstruction Costs, Adjusted (\\'000 US$)'],\n",
    "        'InsuredDamage': row[1]['Insured Damage (\\'000 US$)'],\n",
    "        'InsuredDamageAdj': row[1]['Insured Damage, Adjusted (\\'000 US$)'],\n",
    "        'TotalDamage': row[1]['Total Damage (\\'000 US$)'],\n",
    "        'TotalDamageAdj': row[1]['Total Damage, Adjusted (\\'000 US$)'],\n",
    "        'CPICode': f'C_{row[1][\"Start Year\"]}',\n",
    "        'EntryDate': row[1]['Entry Date'],\n",
    "        'UpdatedDate': row[1]['Last Update']\n",
    "    })\n",
    "\n",
    "disaster_df = pd.DataFrame(disaster_list)\n",
    "disaster_df.to_csv('Datasets/CleanedDatasets/Disaster.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7666f09-c84c-4ef2-9511-0305971afdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "To confirm re-creation of tables type -> RECREATE TABLES:  \n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"To confirm re-creation of tables type -> RECREATE TABLES: \")\n",
    "\n",
    "if user_input == \"RECREATE TABLES\":\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True, \n",
    "                                        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)\n",
    "    \n",
    "    dataset = None\n",
    "    dataset_id = f\"{client.project}.eu_disaster\"\n",
    "    \n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_id)\n",
    "    except Exception as e:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset = client.create_dataset(dataset, timeout=30)\n",
    "    \n",
    "    table_names = [file_name.split(\".\")[0] for file_name in os.listdir(\"Datasets/CleanedDatasets/\")]\n",
    "    \n",
    "    for table in table_names:\n",
    "        table_id = f\"{dataset_id}.{table}\"\n",
    "        table_file_path = f\"Datasets/CleanedDatasets/{table}.csv\"\n",
    "    \n",
    "        with open(table_file_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
